* Types of data
  * Application data
    * Source code and the environment
    * Written & provided by the developer
    * Added to image and container in the build phase (`docker build`)
    * Fixed: Can't be changed once the image is built
    * Application code should be read-only, hence why it's stored in an image
  * Temporary app data
    * Entered user input, for example
    * Data that is fetched/produced while the application (container) is running
    * Stored in memory (saved to a variable in our code) or in a temp file
    * Dynamic and changing, but cleared regularly
    * Temp data should be both read-write only while app is running hency why it's stored in the container
    * It's okay if we lose this data
  * Permanent app data
    * User accounts, for example
    * Data that needs to persist
    * Also data tahis is fetched/produced in a running container
    * Stored in files or a database
    * Must not be lost if container stops/restarts
      * The data should even survive the deletion of a container
    * Permanent data should be both read-write only and stored in containers with the help of volumes 

* The container file system is isolated from our local machine's file system. When we copy our project's file system in the Dockerfile, we're providing a current snapshot of our local file system. Once the container is created, there's no connection its file system and our local's file system. So if a file or folder is created in the container, it will not be reflected in our local file system.

* Just because a container is stopped doesn't mean its file system is erased. This will only happen you remove (delete) the container

* When a file is generated in a container, the container does not write this file into the image; it only writes it into its own container layer. Therefore, when the container is removed, we're only left with the image, whose file system wasn't touched because it's read-only.

* Multiple containers based on the same image are isolated from each other.

* Volumes
  * Helps with persisting data
  * Folders on your host machine's hard drive which are mounted ('made available', mapped) into containers
    * i.e. folders on your host machine which you make Docker aware of and which are then mapped to folders inside of a Docker container
  * You can connect a folder inside of the container to a folder outside of a container on your host machines
  * Changes in either folder will be reflected in the other one.
    * So if you add a file on your host machine, it is accessible inside of the cotnainer, and vice versa

* Volumes persist if a container shuts down. If a container (re-) starts and mounts a volume, any data inside of the volume is available in the container

* A container can write data into a volume and read from it

* `VOLUME` instruction
  * Add this to Dockerfile which takes an array of strings where we can specify the different paths inside of the container file system which we want to persist

* Types of volumes
  * Anonymous volumes and named volumes
  * For both, Docker sets up a folder/path on your host machine, exact is unknown to you
  * Only way to get access to volumes with the `docker volume` command.
    * `docker volume ls` -> list all volumes created
      * VOLUME NAME is autogenerated (anonymous volume) if no name is assigned
  * Anonymous volumes exists as long as our container exists
  * With named volumes, volumes will survive when a container shuts down. The folders on your hard will survive. Therefore if you start new contaienrs after shutdown, the volumes, the folders & its data will still be available.
    * Great for data which should be persistent but which you don't need to edit directly (because you don't have access to that folder on your host machine)
  * You can't create named volumes inside of a Dockerfile. You have to create a named volume when you run a container
    * When running the `docker run` command add the `-v` option where you'll provide the name of the volume (your choice) followed by the path inside the container file system which we want to save, separated by a colon.
    * Example
      * `docker run -v feedback:/app/feedback feedback-node`
        * `feedback` -> name of the volume
        * `/app/feedback` -> path inside the container file system
        * `feedback-node` -> image
  * Named volumes aren't attached to a container; anonymous volumes are
  * If you start a container without the `--rm` option, an anonymous volume would NOT be removed, even if you remove the container

* To clear anonymous volumes, you run `docker volume rm [VOLUME_NAME]` or `docker volume prune`.

* Bind Mounts
  * Whenever we change anything in our source code, those chnages aren't reflected in the running container unless we rebuild the image
  * This can get quite cumbersome, having to rebuild the image each time a change is made, especially during development
  * Bing mounts solve this problem
  * Volumes are managed by Docker and we don't know where on our machine's file system they are located. For bind mounts, we define a folder/path on our host machine to be mapped to the container's internal file system.
  * Perfect palce to put our source code
  * Have to make sure the container is aware of that and the source code is not used from `COPY` instruction's snapshot in the Dockerfile
  * Can't add bind mounts in the Dockerfile because it's specific to the container, not to the image
  * Set up the bind mount when run a container
    * When running the `docker run` command, add the `-v` option where you'll provide the absolute path of the folder you want to mount from your local machine, followed by the folder inside of the container where you copied your soruce code to (in the COPY instruction), separated by a colon
      * Consider putting the arguments in quotes to ensure that it doesn't break if your path contains special characters or whitespace
      * Make sure Docker has access to the folder that you're sharing as a bind mount. Go to Docker -> Preferences -> Resources -> File Sharing and add the parent folder of the fold you're sharing
        * More than likely won't have to do this because, by default, Docker already lists the "/Users" folder which is all that's needed
  * Example
    * `docker run -v $(pwd):/app feedback-node`
      * `$(pwd)` -> shortcut for local path where the source code exists
      * `/app` -> container path where the source code exists
      * `feedback-node` -> image

* When we use bind mounts, we're binding everything in our local directory to the /app directory in the container, which essentially means we're overwering the /app folder inside the container with our local folder. Therefore, the COPY (and RUN) instructions in the Dockerfile become worthless.
  * For example, when the RUN instruction does an "npm install", a node_modules folder is created in the container. On our local machine, we don't have that node_modules folder because we don't run npm install there.

* Docker doesn't overwrite our local file system when using bind mounts, only the container's file system

* To prevent certain files or folders from being overwritten, add an anonymous volumes to the docker run command or you can add the VOLUME instruction to your Dockerfile
  * Example: using the `-v` option on `docker run`
    * `docker run -v $(pwd):/app -v /app/node_modules feedback-node`
      * `-v $(pwd):/app` -> bind mount
      * `-v /app/node_modules` -> anonymous mount we don't want to overwrite when using the bind mount
      * `feedback-node` -> image

* Node specifc trick
  * To add nodemon to your container (watches for changes to nodejs code)
    1. Add `"devDependencies"` node to package.json & add nodemon with version(`"nodemon": "2.0.16"`)
    2. Add `"scripts"` node to packages.json & add `"start" : "nodemon [file-name.js]"`
    3. Change the `CMD` instruction to the Dockerfile to `CMD ["npm", "start"]`
    4. Changes will now be propogated to the container

* The running application should not have the ability to change the files in the /app directory when using bind mounts. In that case, we should turn the bind mount into a read-only volume

* By default, volumes are read-write, meaning that the container is able to read data from there and write data to them.

* To enforce a bind mount to be read-only, add `:ro` after the internal path in the docker run command
  * Example
    * `docker run -v $(pwd):/app:ro`
  * Still will be able to change files on host machine, only affects the application running in the container
  * If there any folders that are nested in the host machine's path of the bind mount that you don't want to restrict write access to, either add a named-volume or anonymous volume for that folder, depending on your needs
    * In other words, if we specify a volume that is inside another volume we've defined, then the more specific volume overrides the least specific volume
      * Example
        * Say I have this read-only bind mount: `docker run -v $(pwd):/app:ro`. With just this volume, everything in the container's /app route will be read-only. If I add another volume that's more specific, meaning a directory within /app such as `-v /app/temp`, then that directory will not be read-only (as well as overwritten)
    * To ensure that the overriding occurs, this can only be done in the `docker run` command, not the Dockerfile

* Bind mounts won't show up in volumes list (`docker volume ls`) because they're not managed by Docker

* `docker volume rm [volume]`
  * Removes one or more volumes that isn't in use by a running container
  * Removing a volume causes all of the data it contains to be lost

* `docker volume prune`
  * Removes all volumes at once

* Bind mounts should only be used in development

* To restrict copy files or folders during the COPY instruction, add a `.dockerignore` file to the root project directory
  * Add anything which isn't required by your application to execute
  * Dockerfile, node_modules, .git are good files 

* `ARG` instruction
  * Build-time arguments, not available in application code
  * Variables in your Dockerfile which you can use to plug different values into certain Dockerfile instructions based on arguments that are provided with the `--build-arg` option when you run `docker build`
  * Setting arguments in Dockerfile
    * Example
      * `ARG DEFAULT_PORT=80`

* You can seet a default vault when you're defining an argument and change it when you run `docker build --build-arg`
  * Example: say I have `ARG DEFAULT_PORT=80` defined in my Dockerfile
    * docker build --build-arg DEFAULT_PORT=8000
  * Useful for when you want to lock certain values when you build an image
  * Allows you to build different images in a flexible way without having to change the Dockerfile

* `ENV` instruction
  * Available inside of Dockerfile and in application code
  * Can set via the ENV instruction in Dockerfile or via the `--env` or `-e` option on `docker run`
  * With ENV instruction, you can set environment variables telling Docker that you're expecting an environment variable to be existing, then provide concrete values on the --env option on docker run.
  * Setting environment variables in Dockerfile
    * Example
      * `ENV PORT 80`
      * To use the variable thorughout the Dockerfile, use a dollar sign followed by the name
        * Example
          * `ENV PORT 80`  
            `EXPOSE $PORT`  
  * Setting environment variables in `docker run` command
    * Example
      * `docker run -e PORT=80`
    * If you have a file with env variables in it, use the `--env-file` option the docker run command and point to that file
      * Example: assuming I have afile called .env in root directory
        * `docker run --env-file ./.env
      * If you use a separate file, environment variables whether they contain sensitive information or not, will not become a part of the image since you point at that file when you run docker run. Just don't commit this file to source control

* Arguments and environment variables allow you to create more flexible images and containers because you don't have to hardcode everything into the container & images. Instead, you can set it dynamically when you build an image or when you run a container.

* Best practice to define ENV and ARG instructions where you would need it thereafter.

